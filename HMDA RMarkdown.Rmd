---
title: "HMDA Analysis"
author: "Alexander Proctor"
date: "May 18, 2018"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## What is the HMDA?

In 1975, Congress passed the Home Mortgage Disclosure Act (HMDA for short) to address concerns that financial institutions were unfairly denying loans to applicants from certain areas, or making decisions based on protected classes. The act does not impose any financial requirements on the institutions, instead requiring them to publish data for each loan granted or denied. Thus, the impetus is on the general public to determine if lenders are acting unethically.
 
We will be working with data disclosed in 2008 - at the height of a major financial crisis primarily caused by an overabundance of risky home loans. While I largely won't be addressing the crisis in my analysis, its presence may be seen in explorations of when loans are denied.

Data specifications for HMDA-disclosed data are available from the Federal Financial Institutions Examination Council (FFIEC). Specifications for 2008 specifically were retrived from https://www.ffiec.gov/hmda/pdf/spec2008.pdf; general codes (such as those for action_type) were retrieved from https://www.ffiec.gov/hmda/pdf/code.pdf.
 
## Loading the data

First, we need to load the data into R using the read_csv function.

The data was published as a single csv file containing roughly 17 million entities (individual loans) across 45 attributes. Since this is far too much data for R to handle (loading from csv takes well over an hour) I will be working with a random sample of 170,000 loans (1% of the data).
```{r loading, eval=FALSE}
library(tidyverse)

filename <- "C:/Users/Alex/Desktop/CMSC320/FINAL/2008HMDALAR.csv"

cols <- c("year","respondent_ID","agency_code","loan_type","property_type", "loan_purpose","occupancy","loan_amount","preapproval","action_type","MSA/MD","state_code","county_code","census_tract_number","applicant_ethnicity","co_applicant_ethnicity","applicant_race_1","applicant_race_2","applicant_race_3","applicant_race_4","applicant_race_5","co_applicant_race_1","co_applicant_race_2","co_applicant_race_3","co_applicant_race_4","co_applicant_race_5","applicant_sex","co_applicant_sex","applicant_income","purchaser_type","denial_reason_1","denial_reason_2","denial_reason_3","rate_spread","HOEPA_status","lien_status","edit_status","sequence_number","population","minority_population_pct","HUD_median_family_income","tract_to_MSA/MD_income_pct","owner_occupied_unit_num","1-to-4-family_unit_num","prior_to_2004")

hmda_2008_full <- read_csv(file=filename, col_names = cols)

hmda_smpl <- hmda_2008_full %>% sample_frac(0.01)
hmda_smpl
```

```{r hidden_load, include=FALSE, echo=FALSE}
# This was prepared prior to knitting using the code above; I am re-loading it from a separate csv now
library(tidyverse)
hmda_smpl <- read_csv("C:/Users/Alex/Desktop/CMSC320/FINAL/HMDA_sampled.csv")
hmda_smpl
```

# Data wrangling

This data needs a small amount of work to become tidy. It is already in the form of entity x attributes - one row per loan, and one column per data point. We will need to convert encoded numbers into NA or factors later, and we will need to pare down some of the data to make it usable, and modify datatypes from characters to integers.
```{r wrangling}
# removing extraneous info
hmda_smpl <- hmda_smpl %>%
  select(loan_amount, action_type, state_code, county_code, race=applicant_race_1, sex=applicant_sex,
         income=applicant_income, median_income=HUD_median_family_income)

# replacing nonexistant code with NA and removing
hmda_smpl <- hmda_smpl %>%
  mutate(sex=ifelse(sex != 1 & sex != 2, NA, sex),
         race=ifelse(race == 7, NA, race)) %>%
  na.omit(hmda_smpl)

# fixing column specs
hmda_smpl <- hmda_smpl %>% 
  
  # removing leading zeroes
  mutate(loan_amount=sub("^0+", "", loan_amount),
         income=sub("^0+", "", income),
         median_income=sub("^0+", "", median_income)) %>%
  
  # multiplying by $1,000
  mutate(loan_amount=paste(loan_amount,"000",sep=""),
         income=paste(income,"000",sep="")) %>%
  
  # specifying column datatype
  mutate(loan_amount=as.integer(loan_amount),
         income=as.integer(income),
         median_income=as.integer(median_income))
  
```
  
# Plotting

With our data in a tidier form, we can start to look deeper. Let's look at how loans vary over geography!

R's usmap library allows us to display geographical data for the United States. The plot_usmap() function requires a data frame with exactly two columns: fips and values. FIPS (Federal Information Processing Standards) state codes are unique two-digit numbers mapped to states/territories, while 5-digit state+county codes do the same on a finer scale. Plot_usmap() takes these fips/value pairs and converts them into a ggplot-compliant map object. Luckily, our data is already in this form; we only need to combine the columns.

Just to start, let's show how to use this tool with a simple median income x county map:
```{r map1}
library(usmap)

# combining codes
hmda_smpl <- hmda_smpl %>%
  mutate(fips=paste(state_code, county_code, sep=""))

# selecting unique data - one value per fips code
income_data <- hmda_smpl %>%
  select(fips, value=median_income) %>%
  unique.data.frame()

# mapping
plot_usmap(data=income_data, regions="counties", values="value") +
  scale_fill_continuous(low = "white", high = "darkgreen", name = "Median Income") + 
  labs(title = "Median income by county") +
  theme(legend.position = "right")
```
We are missing some data from the more sparsely-populated counties - less people means less houses means less home loans, after all.

We can see some tight concentrations of high income in certain counties, but our data is in largely inscrutable FIPS form. Let's use the county.fips database from the maps package to convert to a readable form.

```{r fips}
library(maps)
data(county.fips) # loading the data

hmda_smpl %>%
  arrange(desc(median_income)) %>%
  select(fips, median_income) %>%
  unique.data.frame() %>%
  slice(1:10) %>%
  mutate(fips=as.integer(fips)) %>%
  left_join(county.fips, by="fips") %>%
  select(median_income, county=polyname)
```
Looks like the DC suburbs and Bay Area top the list, as expected.

Let's use this to make some more interesting observations. How does the average amount of money requested change by county? Are there any counties where the average loans requested are substantially higher or lower than median income? 
```{r map2}
loan_amt_data <- hmda_smpl %>%
  group_by(fips) %>%
  mutate(value=mean(loan_amount)) %>%
  select(fips, value) %>%
  unique.data.frame()

plot_usmap(data=loan_amt_data, regions="counties", values="value") +
  scale_fill_continuous(low = "white", high = "red", name = "Average Loan Amount") + 
  labs(title = "Mean loan requested by county") +
  theme(legend.position = "right")
```
Something doesn't look right here. Let's take a closer look at the data. 

```{r remove_outliers_1}
hmda_smpl %>%
  mutate(fips=as.integer(fips)) %>%
  group_by(fips) %>%
  mutate(avg_loan=mean(loan_amount)) %>%
  ungroup() %>%
  arrange(desc(avg_loan)) %>%
  select(fips, avg_loan) %>%
  unique.data.frame() %>%
  slice(1:10) %>%
  left_join(county.fips, by="fips") %>%
  select(avg_loan, county=polyname)
```
Elmore County, Alabama. Either the city of Montgomery underwent a major rebuild in 2008, or there is an error in the data. Let's remove any outliers just to be safe.

```{r removing_outliers}
# calculating z scores and removing anything more than 2 standard deviations from the mean
hmda_standard <- hmda_smpl %>%
  group_by(fips) %>%
  mutate(mean_loan=mean(loan_amount)) %>%
  mutate(sd_loan=sd(loan_amount), sd_income=sd(income)) %>%
  mutate(z_loan=(loan_amount-mean_loan)/sd_loan, z_income=(income-median_income)/sd_income) %>%
  filter(z_loan > -2 & z_loan < 2 & z_income > -2 & z_income < 2) %>%
  filter(income > 1000 & income < 300000) # extra uncaptured outliers
  
loan_data <- hmda_standard %>% 
  group_by(fips) %>%
  mutate(mean_loan=mean(loan_amount)) %>%
  ungroup() %>%
  select(fips, value=mean_loan) %>%
  unique.data.frame()
  
plot_usmap(data=loan_data, regions="counties", values="value") +
  scale_fill_continuous(low = "white", high = "red", name = "Average Loan Amount") + 
  labs(title = "Mean loan requested by county") +
  theme(legend.position = "right")

loan_data %>%
  mutate(fips=as.integer(fips)) %>%
  arrange(desc(value)) %>%
  select(fips, value) %>%
  slice(1:5) %>%
  left_join(county.fips, by="fips") %>%
  select(value, county=polyname)
```
This makes more sense. Loans are largely the same in the middle of America, with larger amounts requested primarily in California (whose housing prices typically top the list).

Let's take one last look at the geographical data. How does the ratio of loan-requested and individual income vary by area? Are people being forced to take on much riskier loans in certain areas, or are salaries largely commensurate with housing prices?
```{r ratio}
ratio_data <- hmda_standard %>%
  filter(income > 1000) %>% # single outlier
  group_by(fips) %>%
  mutate(ratio=loan_amount/income) %>%
  mutate(value=mean(ratio)) %>%
  ungroup() %>%
  select(fips, value) %>%
  unique.data.frame()

plot_usmap(data=ratio_data, regions="counties", values="value") +
  scale_fill_continuous(low = "white", high = "red", name = "Average Loan:Income Ratio") + 
  labs(title = "Loan:Income ratio by county") +
  theme(legend.position = "right")

ratio_data %>%
  mutate(fips=as.integer(fips)) %>%
  arrange(desc(value)) %>%
  select(fips, value) %>%
  slice(1:5) %>%
  left_join(county.fips, by="fips") %>%
  select(fips, value, county=polyname)

```
The ratio seems to be largely the same, with a few pockets of high loan:income plausibly caused by a lack of data (the highest seem to be sparsely populated). 

While there does not appear to be much variation over area, we can see there does appear to be a relationship between income and loan amount. This will lead us into our next section.
```{r ratio2}
hmda_standard %>%
  filter(income < 200000 & income > 1000) %>%
  ggplot(aes(x=income, y=loan_amount)) +
  geom_point() + geom_smooth(method=lm)
```

# Modeling

With our exploration out of the way, we can start to model our data.

Let's move on to the reason why the HMDA was passed - attempting to identify unfairness or bias in lending behaviors. To capture this, we need to modify our data slightly. In the FFIEC code specifications, we can see what action types correspond to.

Types 1 and 2 are loans that have been explicitly accepted - type 1 is complete, and type 2 is pending the applicant's acceptance. Type 6 was granted by a prior institution and later transferred to the reporting one.

Types 3 and 5 are explicitly denied. Type 3 comes with a variety of given reasons, but 5 is rejected due to lack of information.

Type 4 was withdrawn by the applicant.

With this, we can modify our data to capture when loans are explicitly denied. Let's see if we can perform a logistic regression on loan denial using a few different factors.
```{r denied_model_1}
library(broom)

# adding the application denied flag
hmda_mod <- hmda_standard %>%
  mutate(application_denied=ifelse(action_type==3|action_type==5, 1, 0))

# building the model
income_model <- glm(application_denied~z_income, data=hmda_mod, family="binomial")
income_model %>% tidy()
```
From this, we can see that an income that is exactly average for the US - one with a z-score of 0 - has less-than-50/50 odds of having a loan denied. In 2008, the average American was able to secure the loan they requested at better than chance odds. But log-odds are awkward to work with. Let's convert to probability:
```{r denied_model_1_cont}
# function to convert from log-odds to probability
b0 <- coef(income_model)[1]
b1 <- coef(income_model)[2]
prob_denied <- function(d) {
  exp(b0+(b1*d))/(1+exp(b0+(b1*d))) 
}
prob_denied(0)
```
From this, we can see that the average American in 2008 had roughly a 28% chance of having their loan denied. We can plot this chance over income to get a better picture:
```{r denial_plot_1}
hmda_mod %>%
  mutate(prob_denied=prob_denied(z_income)) %>%
  ggplot(aes(x=z_income, y=prob_denied)) +
  geom_point()
```
Our model matches the data perfectly! We can see that someone with an income 2 standard deviations below the mean still has a 50% chance of getting a loan; this rate may have had something to do with the subprime mortgage crisis occurring during the time this data was collected.

Let's try our modified data out for regional bias. We'll look at state-by-state data this time, to account for small numbers of measurements per county in the sparsely-populated Midwest.
```{r denial_area}
denial_data <- hmda_mod %>%
  group_by(state_code) %>%
  mutate(value=mean(application_denied)*100) %>%
  select(fips=state_code, value) %>%
  unique.data.frame()

plot_usmap(data=denial_data, values="value") +
  scale_fill_continuous(low = "white", high = "red", name = "% loans denied") + 
  labs(title = "% loans denied by county") +
  theme(legend.position = "right")
```
West Virginia and Florida seem to have a disproportionately-high number of loans denied. 

Let's look now at how loan denial varies by race - a concern big enough that the HMDA was amended in 1989 to include this data. We'll have to first modify the data from the single-digit code to a descriptive factor, before grouping and calculating averages.
```{r race}
hmda_mod %>%
  
  # re-working race from int to factor
  mutate(race=as.character(race)) %>%
  mutate(race=ifelse(race==1, "amerindian", race)) %>%
  mutate(race=ifelse(race==2, "asian", race)) %>%
  mutate(race=ifelse(race==3, "black", race)) %>%
  mutate(race=ifelse(race==4, "hispanic", race)) %>%
  mutate(race=ifelse(race==5, "white", race)) %>%
  mutate(race=ifelse(race==6, "other", race)) %>%
  
  # filtering out non-accepted action types
  filter(action_type != 4 & action_type != 6) %>%
  
  # finding % loans denied by race
  group_by(race) %>%
  summarize(pct_denied=mean(application_denied)*100) %>%
  select(race, pct_denied) %>%
  unique.data.frame() %>%
  
  # plotting the data
  ggplot(aes(x=race, y=pct_denied)) + geom_histogram(stat="identity")
```
This graph appears to show a strong bias - Native Americans and African Americans are denied loans at higher rates than others. But is this statistically significant? Let's attempt to show this via a regression model.

It's important to note that building a predictive model for race is a Bad Idea for ethical reasons - if this model were used to decide who to grant loans to, it would be terrible. I am using this only as a descriptive tool to show that bias has already occurred.

In order to capture this logarithmically, we'll need to convert the race attribute into what is known as "one-hot-encoding". As models need numerical data to work on, we will simply create one column for each recorded race, and set its value to either 1 or 0.
```{r race_reg}
one_hot <- hmda_mod %>%
  mutate("amerindian"=ifelse(race==1, 1, 0),
         "asian"=ifelse(race==2, 1, 0),
         "black"=ifelse(race==3, 1, 0),
         "hispanic"=ifelse(race==4, 1, 0),
         "white"=ifelse(race==5, 1, 0),
         "other"=ifelse(race==6, 1, 0))

race_model <- glm(application_denied~amerindian+asian+black+hispanic+white, data=one_hot, family="binomial")
race_model %>% tidy()
```
We can see that there is a significant bias - the p-value for each race estimate is less than the required 0.05 to reject our null hypothesis of no bias. It seems as if the HMDA still needs public interest in order to bring this to light.
